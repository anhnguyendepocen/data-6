{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating an RCT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the lab 15!\n",
    "\n",
    "In today's lab, we'll replicate some results from a recent randomized controlled experiment.\n",
    "\n",
    "In the Second Liberian Civil War, which lasted from 1999 to 2003, many children were recruited as soldiers.  By 2009, many of these ex-fighters were working in illegal industries or as mercenaries.\n",
    "\n",
    "An organization called Action On Armed Violence wanted to know whether a program of training and monetary aide could help ex-fighters reintegrate into Liberian society.  Such programs have had mixed results in other contexts, but most of the available evidence was from observational studies, not RCTs.  They identified a candidate group of ex-fighters, randomly assigned them to treatment and control groups, and offered their assistance program to the treatment group.\n",
    "\n",
    "Our goal is to determine whether the assistance program actually improved outcomes for the people in the experiment.\n",
    "\n",
    "The data are available [here](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/11R0LX&version=2.1) and the paper [here](https://www.povertyactionlab.org/sites/default/files/publications/994_138_Can-Employment-Reduce-Lawlessness-and-Rebellion_June2015.pdf).  The data come straight from the first link, but we've done some cleaning to make it easier to analyze.\n",
    "\n",
    "You should also be aware that there are many additional questions that go into the design and analysis of an RCT.  For example, some of the people in the experimental group declined to participate, which could become a confounding factor in the analysis if not handled properly.  (Why?)  Many details like that are discussed in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this cell to set up the notebook, but please don't change it.\n",
    "\n",
    "# These lines import the Numpy and Datascience modules.\n",
    "import numpy as np\n",
    "from datascience import *\n",
    "from lab_functions import *\n",
    "\n",
    "# These lines do some fancy plotting magic.\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', FutureWarning)\n",
    "\n",
    "# These lines load the tests.\n",
    "import otter\n",
    "grader = otter.Notebook()\n",
    "Table.interactive_plots()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warmup: Flipping a Coin\n",
    "\n",
    "Before we begin, let's quickly review hypothesis testing. Recall that in hypothesis testing, we have a specific view of the world (our \"null hypothesis\"). We perform a study by collecting a sample - giving us actual data. We can use that data to test the validity of our view of the world. If the data is consistent with our view of the world, then we say we \"fail to reject\" the null hypothesis. If the data is not consistent with our view (i.e. the observed data is very unlikely to occur under our null hypothesis), then we would \"reject\" the null hypothesis. Like any scientist, we want to form a hypothesis, make observations and collect data, and then make conclusions about the world based on the data, taking into account random chance, the study's design, and other factors that may affect our results.\n",
    "\n",
    "So, let's begin with an example. \n",
    "\n",
    "Meghana is flipping a coin. She thinks it is fair, but is not sure. She flips it 10 times, and gets heads 9 times. She wishes to determine whether the coin was actually unfair, or whether the coin was fair and her result of 9 heads in 10 flips was by random chance.\n",
    "\n",
    "In the cell below, \n",
    "\n",
    "(1) Write a potential null hypothesis, or a model that we can simulate under.\n",
    "\n",
    "(2) Write an alternative hypothesis or model. You do not necessarily need to be able to simulate under this model.\n",
    "\n",
    "(3) What would we measure? What is a good test statistic for this experiment?\n",
    "\n",
    "Try writing the answer before you read the true hypotheses in the cell below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Type your answer here, replacing this text.*\n",
    "\n",
    "1. \n",
    "\n",
    "2. \n",
    "\n",
    "3. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we will be measuring the absolute difference of heads and tails in 10 flips: abs(num_heads-num_tails).\n",
    "\n",
    "A proper null hypothesis would be \"The coin is fair; the probability of getting a heads is 50% and every difference is due to chance.\" Therefore, under that model, we would expect our test statistic, the absolute difference, to be around 0. \n",
    "\n",
    "Alternatively, a proper alternative hypothesis would simply be the opposite of this - \"the coin is unfair, and the probability of getting a heads is something other than 50%.\" We would expect it to be some value greater than 0, and we would need the histogram (shown below) to simulate how likely values other than 0 occur.\n",
    "\n",
    "We then simulate under the null hypothesis and receive the following histogram.\n",
    "\n",
    "![Coin Histogram](coin_hist.JPG)\n",
    "\n",
    "Is the observed statistic (Meghana's 9 heads out of 10 flips) described previously consistent with the model we simulated under? Explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Type your answer here, replacing this text.*\n",
    "\n",
    "4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Action on Armed Violence Study\n",
    "\n",
    "Now, let's get back to the study.\n",
    "Run the following cell to load the data in a table called `original`. The raw dataset is too large for our analysis (there are over a thousand columns!), so we shortened it to the columns that we will use for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "original = Table().read_table(\"farming.csv\")\n",
    "original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data\n",
    "The `original` table presents a few challenges.  We should clean it up a bit before we proceed. \n",
    "\n",
    "For our analysis, we're going to look only at two of those columns:\n",
    "  * `assigned_final`: Whether the person was in the treatment or the control group.  1 means that the person received the treatment, and 0 means that they did not and were in the control group.\n",
    "  * `raiseanintfut_dum_end`: Whether the person expressed interested in raising animals at the end of the trial period.  1 means they expressed interest, and 0 means they didn't.  (`\"raiseanintfut\"` is short for \"raise animals in the future.\"  `\"dum\"` is short for \"dummy,\" which just means the variable is coded as 0 or 1.  `\"end\"` means the variable was measured at the end of the experiment.)\n",
    "\n",
    "Here are some problems:\n",
    "1. Some of the people didn't report their interest.  Their `raiseanintfut_dum_end` are labeled as `nan`, which stands for \"not a number.\"  This will cause a problem for any analysis of that column.  They can be removed by choosing only rows where `raiseanintfut_dum_end` is greater than or equal to 0. (What table method lets us filter a dataset?)\n",
    "\n",
    "\n",
    "2. The existing column names are terrible.  They should be called `\"Treatment\"` and `\"Interested in raising animals\"`, so we should relabel them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Produce a table called `farming` that is a cleaned-up version of `original`.  You should address the two main concerns described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "farming = \n",
    "farming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "Find the **proportion** of people interested in raising animals in the treatment and control groups.  **Find a way to produce a table containing these values with a single call to the `tbl.group` method.**  Name your table `proportions`.\n",
    "\n",
    "*Hint:* If two people are 1s and three are 0s, then .2 of the people are 1s.  That's also the *average* of the array [1, 1, 0, 0, 0].\n",
    "\n",
    "Then, use the `proportions` table to create a bar chart showing the difference between the treatment and control groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proportions = \n",
    "proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Draw your bar chart here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Is there a pattern you observe in the data? What do you notice about the two groups?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is there really an effect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it appears there is a difference - that ex-fighters who received the treatment seemed highly interested (~90%) in raising animals - our experimental design meant that individuals were assigned randomly to the treatment and control groups. Therefore, it is therefore possible that the result you is due to chance. How do we determine that the treatment really had an effect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "Define appropriate null and alternative hypotheses that you could test to guard against this possibility.\n",
    "\n",
    "Remember - our question that we want to answer is: Does this treatment (a training and monetary aid program) improve outcomes for ex-fighters, measured in the likelihood of interest in raising animals?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Generally, we want a large sample when we perform a hypothesis test. In this case, we will compare the results we received in the observed data, in `farming`, with simulated data that assumes that the treatment has no effect.\n",
    "\n",
    "But first - what size should our simulated samples be?  Calculate it using Python code.\n",
    "\n",
    "*Hint:* If we are performing a simulation with some data, we usually want to make sure the simulation follows a similar setup to the observed dataset (i.e. the same sample size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_size = \n",
    "sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grader.check('q5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've determined the sample size, how can we run many simulations under the null hypothesis? One approach is to create 1 sample at a time under the null hypothesis/model, and then compute the test statistic for that simulated sample.\n",
    "\n",
    "Running simulations in a hypothesis test is a multi-step process. The steps are as follows:\n",
    "\n",
    "1) Calculate the observed test statistic\n",
    "\n",
    "2) Simulate creating many samples under the null hypothesis/model\n",
    "\n",
    "3) Calculate the test statistic for each simulation under the null model\n",
    "\n",
    "4) Compare the observed test statistic to the simulated test statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 \n",
    "\n",
    "First, we will write our test statistic function, called `test_statistic_function`. In this case, since we are comparing a treatment and control group, we will use **the absolute difference between the proportion of people interested in raising animals that are in treatment and control groups.** In other words, abs(treatment_prop - control_prop). Therefore, the closer the value is to 0, the more similar the groups are. \n",
    "\n",
    "Therefore: `test_statistic_function` should take in a 2-column table in the same format as `farming`, and return the absolute difference in the proportions for the 2 groups. \n",
    "\n",
    "*Hint:* First, you should separate the sample into treatment and control groups. Then, you should find the proportion of people who are interested in raising animals in the treatment group. Do the same for people in the control group. The code should be similar to what we've done previously in Question 2. Lastly, find the difference in proportions between both groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your test_statistic_function function here.\n",
    "def test_statistic_function(sample):\n",
    "    ...\n",
    "    return ...\n",
    "\n",
    "\n",
    "## If your function works, this should output the observed test statistic.\n",
    "observed = test_statistic_function(farming)\n",
    "observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grader.check('q6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "Now, we want to sample under the null hypothesis (i.e. assume that the treatment has no effect), so we can compare our observed result to what we think *should* happen. We created a function for you that will help do this called `simulate_under_null`, which takes in the original sample (`farming`), simulates under the null 1000 times, and outputs an array of 1000 test statistics, each calculated using a simulation under the null.\n",
    "\n",
    "**Note:** The code that goes into `simulate_under_null` is outside of the scope of this course. However, if you're interested, it performs an A/B test (described [here](https://www.inferentialthinking.com/chapters/12/1/AB_Testing.html)), which is a technique used to compare two groups or samples. You will learn this in Data 8, if you decide to take it! This process is also used very often in web design to see how users respond to various stimuli. \n",
    "\n",
    "Using the `simulate_under_null` function and `farming`, assign `sim_stats` to an array of the 1000 simulated statistics. Then, using that data, create a table so that you can make a histogram showing the test statistics under the null. It may take a second to load. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sim_stats = ...\n",
    "\n",
    "...\n",
    "#Do not delete this line - it will plot the observed statistic as a red dot on your histogram.\n",
    "plt.scatter(observed, 0, color='red', s=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "What do you see? Based on the histogram, is the observed data consistent or inconsistent with the null hypothesis? In other words, does the treatment appear to have an effect? Explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "You're done with Lab 15! \n",
    "\n",
    "To submit this notebook, please download your notebook as a .ipynb file and submit to Gradescope. You can do so by navigating to the toolbar at the top of this page, clicking File > Download as... > Notebook (.ipynb). Then, go to our class's Gradescope page [here](https://www.gradescope.com/courses/136698) and upload your file under \"Lab 15.\" \n",
    "\n",
    "To check your work for all autograded questions, run the cell below. \n",
    "\n",
    "It's fine to submit multiple times, but we will only grade the final notebook you submit for each assignment. Make sure you pass all tests to receive credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For your convenience, you can run this cell to run all the tests at once.\n",
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
